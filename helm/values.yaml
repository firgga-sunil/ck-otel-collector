# --- Port Configuration Quick Reference ---
# This OpenTelemetry Collector is configured to use the following key ports:
#
# - Port 4319 (OTLP gRPC):
#   - OTel Collector: Receives incoming application telemetry via OTLP gRPC.
#   - Kubernetes: Exposed via ContainerPort, ServicePort (named 'otlp' or similar), and HostPort.
#
# - Port 8889 (Main Prometheus Exporter):
#   - OTel Collector: Exports processed application metrics (from OTLP).
#   - Kubernetes: Exposed via ContainerPort and ServicePort (named 'metrics').
#   - ServiceMonitor: Targets the Service port named 'metrics' (i.e., 8889) for Prometheus to scrape.
#
# - Port 13134 (Health Check):
#   - OTel Collector: The health_check extension listens on this port.
#   - Kubernetes: Liveness and Readiness probes target this port on the container to check pod health.
#
# - Port 1777 (pprof):
#   - OTel Collector: The pprof extension listens on this port for performance profiling.
#   - Kubernetes: Exposed via ContainerPort and ServicePort for debugging and profiling.
#
# - Port 8888 (Collector Self-Metrics):
#   - OTel Collector: Exposes its own internal metrics (otelcol_*) on this port.
#   - Kubernetes: Exposed via ContainerPort and ServicePort for Prometheus scraping.
#
# --- End Port Configuration Quick Reference ---


# OpenTelemetry Collector configuration
# mode: daemonset
mode: deployment
replicaCount: 1

# Image configuration
image:
  repository: ck-intel-collector
  tag: latest
  pullPolicy: Never  # Use local image from kind

# Service configuration
service:
  type: ClusterIP #this will work within the same kubernetes cluster.
  enabled: true

# Ports configuration (OpenTelemetry Helm chart schema)
ports:
  otlp:
    enabled: true
    containerPort: 4319
    servicePort: 4319
    protocol: TCP
    appProtocol: grpc
  metrics: # This is for the Prometheus exporter
    enabled: true
    containerPort: 8889
    servicePort: 8889
    protocol: TCP
  # Self-observability ports
  pprof:
    enabled: true
    containerPort: 1777
    servicePort: 1777
    protocol: TCP
  # Collector self-metrics port (for prometheus receiver to scrape)
  # Note: Port name must match ServiceMonitor port reference
  # The Helm chart uses the key name as the Service port name
  self-metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    protocol: TCP
  # Disable other default ports
  otlp-http:
    enabled: false
  jaeger-compact:
    enabled: false
  jaeger-thrift:
    enabled: false
  jaeger-grpc:
    enabled: false
  zipkin:
    enabled: false

# Resource configuration
resources:
  limits:
    cpu: 1
    memory: 2Gi
  requests:
    cpu: 200m
    memory: 400Mi

# ServiceMonitor configuration
serviceMonitor:
  enabled: true
  metricsEndpoints:
    # Application metrics endpoint (from prometheus exporter)
    - port: metrics # This should match the name of the port in the Service
      interval: 30s
      path: /metrics
    # Collector self-observability metrics endpoint (from telemetry)
    - port: self-metrics # Collector's internal metrics (otelcol_*) - matches the deployed Service port name
      interval: 30s
      path: /metrics
  extraLabels:
    app.kubernetes.io/name: otel-collector
    app: ck-otel-collector
    release: ckp

# Health check probes
livenessProbe:
  httpGet:
    path: /
    port: 13134
  initialDelaySeconds: 5
  periodSeconds: 20

readinessProbe:
  httpGet:
    path: /
    port: 13134
  initialDelaySeconds: 5
  periodSeconds: 10

# Collector configuration
config:
  extensions:
    health_check:
      endpoint: "0.0.0.0:13134"
    pprof:
      endpoint: "0.0.0.0:1777"

  receivers:
    otlp: # Only OTLP gRPC receiver is defined
      protocols:
        grpc:
          # Ensure this port matches ports.otlp.containerPort
          endpoint: "0.0.0.0:4319"
        http: null
      
      # Header extraction configuration
      header_extraction:
        enabled: true
        headers_to_extract:
          # Extract ck-domain from header and add as domain resource attribute
          - header_name: "ck-domain"
            attribute_name: "ck_domain"
    
    # Explicitly disable default receivers
    jaeger: null
    zipkin: null
    prometheus: null  # Explicitly disable the default prometheus receiver
     
  processors:
    batch:
      timeout: 1m
      send_batch_size: 16384
    # Explicitly disable default memory_limiter
    memory_limiter: null

    # # Protects the collector from running out of memory.
    # memory_limiter:
    #   check_interval: 1s
    #   # Set limit to ~75-80% of your requested memory (400Mi)
    #   limit_mib: 400
    #   # Spike limit can be a bit higher, e.g., up to the request limit or slightly above
    #   spike_limit_mib: 380

    metricsaggregator:
      # Global configuration - applies to all aggregation rules
      group_by_labels:
        - "ck_service_name"
        - "agent_version"
        - "ck_namespace"
        - "ck_cluster_name"
        - "path_key"
        - "error_code"
        - "mpk"
        - "fc"
        - "flow_id"
        - "quantile"
        # Header extraction attributes
        - "ck_domain"
        # Multi-topic/ipk labels
        - "ipk"
        - "tp"

      output_resource_attributes:
        otel_output_metric: "true"
        otel_output_processor: "metricsaggregator"

      aggregation_rules:
        # Aggregation 1: Sum throughput metrics across all pods
        - metric_pattern: "ck_graph_throughput"
          match_type: "strict"
          output_metric_name: "ck_graph_throughput"
          aggregation_type: "sum"
          preserve_original_metrics: false
          output_metric_type: "sum"

        - metric_pattern: "ck_graph_error_count"
          match_type: "strict"
          output_metric_name: "ck_graph_error_count"
          aggregation_type: "sum"
          preserve_original_metrics: false
          output_metric_type: "sum"

        # Aggregation 2: Average latency metrics across all pods
        - metric_pattern: "ck_graph_latency"
          match_type: "strict"
          output_metric_name: "ck_graph_latency_nanoseconds"
          aggregation_type: "mean"
          preserve_original_metrics: false
          output_metric_type: "gauge"

        - metric_pattern: "ck_cpu_utilization_percent"
          match_type: "strict"
          output_metric_name: "ck_cpu_utilization_percent"
          aggregation_type: "max"
          preserve_original_metrics: false
          output_metric_type: "gauge"

        - metric_pattern: "ck_method_throughput"
          match_type: "strict"
          output_metric_name: "ck_method_throughput"
          aggregation_type: "sum"
          preserve_original_metrics: false
          output_metric_type: "sum"

        - metric_pattern: "ck_method_error_count"
          match_type: "strict"
          output_metric_name: "ck_method_error_count"
          aggregation_type: "sum"
          preserve_original_metrics: false
          output_metric_type: "sum"

  exporters:
    prometheus:
      # Ensure this port matches ports.metrics.containerPort
      endpoint: "0.0.0.0:8889"
      send_timestamps: true 
      metric_expiration: 180m 
      enable_cleanup_api: true 
      resource_to_telemetry_conversion: 
        enabled: true

    # Debug exporter for troubleshooting
    debug:
      verbosity: detailed

  service:
    # Telemetry configuration for collector self-observability
    telemetry:
      logs:
        level: info
        development: false
        encoding: json
        disable_caller: false
        disable_stacktrace: false
      metrics:
        level: detailed
        address: "0.0.0.0:8888"

    extensions: [health_check, pprof]
    pipelines:
      metrics:
        receivers:
          - otlp
        processors:
          - batch
          - metricsaggregator
        exporters:
          - prometheus
          - debug
      # Explicitly disable other pipelines
      logs: null
      traces: null
